{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Launching Dataproc with Cloud Composer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRAC/+SjGDhEvC6fjd6C+j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrinmayeeKulkarni/Cloud-Map-Reduce-and-Distributed-Jobs/blob/master/Launching_Dataproc_with_Cloud_Composer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2rMbZVnJ3Hh",
        "colab_type": "text"
      },
      "source": [
        "### Building an Apache Airflow DAG\n",
        "\n",
        "  * Creating a cloud storage bucket\n",
        "\n",
        "  * Creating BigQuery Dataset : Where all tables will be loaded\n",
        "\n",
        "  * Export data: \n",
        "    * Using New York city yellow cab data for one\n",
        "    * Adding it to a temp table\n",
        "    * Exporting the temp table to google storage\n",
        "\n",
        "  * Creating composer environment : Cloud Composer exposes the Airflow web UI on a public IP address; It also creates security concerns\n",
        "\n",
        "  * Thus, installing necessary Identity Aware Proxy (IAP) requests\n",
        "\n",
        "  * Creating an environment for the DAG bucket\n",
        "\n",
        "  * IAP Authentication: \n",
        "    * In the event of Spark jobs need to be run at an irregular cadence, ther is a need to set up the DAG to be triggered only via a POST to an endpoint. This endpoint will sit behind an Identity Aware Proxy and will need to be called using a service account.\n",
        "    * therefore, need to create servie account post trigger\n",
        "\n",
        "  * Triggering DAG with Python: \n",
        "    * After creating and sourcing the virtual environment it can be seen that by default one of Airflow's dependencies installs a General Public License (GPL) dependency (unidecode). To avoid this dependency SLUGIFY_USES_TEXT_UNIDECODE=yes has to be set in the environment when installing or upgrading Airflow\n",
        "    * Constructing endpoint URL and triggering the DAG: Need webserver URL and CLient ID\n",
        "\n",
        "  * Observing Airflow\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}